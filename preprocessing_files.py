import os
from bs4 import BeautifulSoup
import re

eperkoff_stanfordCoreNLP_Directory_Path = "/home1/c/cis530/hw3/corenlp/stanford-corenlp-2012-07-09"
eperkoff_train_files_path = "/home1/c/cis530/project/train_data"
eperkoff_processed_files_path = "/home1/e/eperkoff/CIS530/project/CIS-530-Project/processed_train_files"
eperkoff_train_file_list = "/home1/e/eperkoff/CIS530/project/CIS-530-Project/file_list.txt"
eperkoff_processed_train_files_as_text_path = "/home1/e/eperkoff/CIS530/project/CIS-530-Project/processed_train_text_files"
eperkoff_test_data_path = "/home1/c/cis530/project/test_data"
eperkoff_test_data_file_list = "/home1/e/eperkoff/CIS530/project/CIS-530-Project/test_file_list.txt"
eperkoff_processed_test_data = "/home1/e/eperkoff/CIS530/project/CIS-530-Project/processed_test_files"

#returns a dict of all the direct children of a directory where the child's name is the key and the exact path is the value
def get_all_files(directory, fileNames):
	for fName in os.listdir(directory):
		fPath = directory + "/" + fName
		if (os.path.isdir(fPath)):
			fileNames+= get_all_files(fPath, fileNames)
		else:
			fileNames.append(fPath)			
	return fileNames

#creates a file with a list of all the file names to send to coreNLP 
def write_list_to_file(file_list, outfile):
	of = open(outfile, 'w')
	for f in file_list:
		of.write(f + '\n')
	of.close()

#makes a system call to coreNLP on the file_list.txt and stores the output in the corenlp_output_dir which contains the coreNLP annotations for files listed in file_list.txt
def preprocess(file_list, corenlp_output_dir):
	command_call_string = "cd " + eperkoff_stanfordCoreNLP_Directory_Path + " && " + " java -cp stanford-corenlp-2012-07-09.jar:stanford-corenlp-2012-07-06-models.jar:xom.jar:joda-time.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -filelist " + file_list +" -outputDirectory " + corenlp_output_dir
	os.system(command_call_string)


#takes in a coreNLP annotated sentence and returns the sentence in string form without punctuation, with the named entities replaced by their tags and with the ends marked with the word STOP
def process_sentence(annotated_sentence):
	#print annotated_sentence
	#soup_sentence = BeautifulSoup(annotated_sentence, "xml")
	token_tag = 'token'
	token_list = annotated_sentence.find_all(token_tag)
	NER_list = []
	sent_string = ""
	for token in token_list:
		word = token.word.string
		NER = token.NER.string
		if NER != 'O':
			#the word is a named entity
			sent_string = sent_string + ' ' +NER
			NER_list.append(str(NER))
		elif re.match('[\w|\']+', word) != None:
			#this means the word is not a named entity or punctuation
			sent_string = sent_string +' ' + word
	#turning repeated named entities into a single one for each different type of named entity i.e. ORGANIZATION ORGANIZATION --> ORGANIZATION
	for named_entity in NER_list:
		#resetting sent_string to have the new set of named entites for this entity
		pattern = named_entity + "( " + named_entity +")+"
		sent_string = re.subn(pattern, named_entity, sent_string)[0]

	sent_string = sent_string + ' STOP '
	return sent_string


#reads input_xml (xml file generated by StanforCoreNLP) and creates output_file that contains the same text but with named entities replaced by their tag and marking the ends of sentences with the word "STOP"
def process_file(input_xml, output_file):
	file_soup = BeautifulSoup(open(input_xml), "xml")
	sentence_tag = 'sentence'
	sentence_list = file_soup.find_all(sentence_tag)
	f = open(output_file, 'w')
	f.write('STOP')
	for sentence in sentence_list:
		clean_sentence = process_sentence(sentence)
		non_ascii_string = ''.join([x for x in clean_sentence if ord(x) < 128])
		f.write(non_ascii_string)
	f.close()





##############RUN THINGS HERE############
#Preprocessing for CoreNLP
'''
#PROCESSING OF TRAIN FILES
file_list = get_all_files(eperkoff_train_files_path, [])
print file_list
write_list_to_file(file_list, eperkoff_train_file_list)
preprocess(eperkoff_train_file_list, eperkoff_processed_files_path)
print "Done preprocessing with CoreNLP"

#Preprocessing for CoreNLP OF TEST FILES
test_file_list = get_all_files(eperkoff_test_data_path, [])
print test_file_list
write_list_to_file(test_file_list, eperkoff_test_data_file_list)
preprocess(eperkoff_test_data_file_list, eperkoff_processed_test_data)


'''
'''
#Preprocessing all train xml files into .txt files 
for f in os.listdir(eperkoff_processed_files_path):
	new_fname = eperkoff_processed_train_files_as_text_path + '/' + f.replace('.txt.xml', '_clean.txt')
	process_file(eperkoff_processed_files_path + '/' + f, new_fname)
print "Done preprocessing all train xml files into .txt files"
	'''

